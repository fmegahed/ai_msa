"""
This file is used to generate chat completions for the Information Quality (adjancey matrix) estimation.

The file builds very logically on: 
  - 01_ai_reliability_multiple_models_ai_exposure.py 
  - we ran it with the models ranked number 1 (tied) on the LMSYS Chatbot Arena, utilizing the Arena Elo
  metric to rank the models. See the screenshot at 'img/chatbot_arena_leaderboard_2024_04_21.png' for more details.
"""

# ------------------------------------------------------------------------------
# Import Libraries
# ----------------

# Standard Libraries
import os
import random
import pickle

# External Libraries
import pandas as pd
import datetime as dt
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

from dotenv import load_dotenv
from itertools import product
from scipy import stats

from langchain.prompts.chat import ChatPromptTemplate
# see https://python.langchain.com/docs/modules/model_io/chat/quick_start/
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic


# ------------------------------------------------------------------------------
# Models:
# -------
models = [
  'gpt-4-turbo-preview', 
  'claude-3-opus-20240229'
  ]

# for saving the data nicely
short_model_names = [
  'gpt4',
  'claude3'
  ]

# ------------------------------------------------------------------------------
# Environment Variables:
# ----------------------

load_dotenv()

openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')


# ------------------------------------------------------------------------------
# System, User and Final Chat Prompt:
# -----------------------------------

system_prompt = f"""
As an expert in large language models (LLMs), information quality and cognitive mapping methodologies, you are tasked with analyzing the causal relationships among various dimensions of information quality as they universally apply to any output generated by LLMs. This task requires evaluating how different aspects of information quality, such as accuracy and trustworthiness, influence each other. Please note that only unidirectional relationships are considered valid, and you should determine the direction and nature of influence between each pair of constructs.

Here are the definitions of each construct:

1. **Accuracy**: The extent to which information is correct and reliable.
2. **Trustworthiness**: The extent to which information is regarded as true and credible.
3. **Objectivity**: The extent to which information is unbiased, unprejudiced, and impartial.
4. **Appropriateness**: The extent to which the volume of information is appropriate for the task at hand.
5. **Relevancy**: The extent to which information is applicable and helpful for the task at hand.
6. **Completeness**: The extent to which information is not missing and is of sufficient breadth and depth for the task at hand.
7. **Conciseness**: The extent to which information is compactly represented.
8. **Consistency**: The extent to which information is presented in the same format.
9. **Understandability**: The extent to which information is easily comprehended.
10. **Interpretability**: The extent to which information is in appropriate languages, symbols, and units, and the definitions are clear.
11. **Flexibility**: The extent to which information is easy to manipulate and apply to different tasks.
12. **Satisfaction**: How the use of information leads to user satisfaction.
13. **Use Intention**: The likelihood that the information will be used in future tasks.

Please complete the upper triangular part of the following 13x13 adjacency matrix to denote the causal relationships among these constructs:

A/B,Accuracy,Trustworthness,Objectivity,Appropriateness,Relevance,Completeness,Conciseness,Consistency,Underestandability,Interpretability,Flexibility,Satisfaction,Use Intention
1. Accuracy,NaN,,,,,,,,
2. Trustworthness,NaN,NaN,,,,,,,
3. Objectivity,NaN,NaN,NaN,,,,,,
4. Appropriateness,NaN,NaN,NaN,NaN,,,,,
5. Relevance,NaN,NaN,NaN,NaN,NaN,,,,
6. Completeness,NaN,NaN,NaN,NaN,NaN,NaN,,
7. Conciseness,NaN,NaN,NaN,NaN,NaN,NaN,NaN,
8. Consistency,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,
9. Underestandability,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,
10. Interpretability,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,
11. Flexibility,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,
12. Satisfaction,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,
13. Use Intention,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN

Your analysis will provide insights into how these factors interact and influence user perceptions of LLM outputs. Ensure to indicate the type and direction of each relationship, or mark it as non-existent where appropriate.
"""

user_prompt = f"""
To complete the adjacency matrix for the 13 dimensions of information quality with respect to outputs from large language models (LLMs), please adhere to the following definitive, non-speculative steps:

0. **Objective Overview**:
   - Your task is to establish definitive relationships among information quality dimensions as they universally apply to any output generated by LLMs. This analysis must be rooted in standard practices and the established definitions of each construct.

1. **Evaluation Criteria**:
   - 'NaN': Not applicable (used for the diagonal and lower triangular part of the matrix).
   - '0': Indicate no causal relationship between the constructs, based on a standard understanding of information quality.
   - 'A+': Mark this when the row construct causally increases the value of the column construct in the context of LLM outputs.
   - 'A−': Use this when the row construct causally decreases the value of the column construct in LLM outputs.
   - 'B+': Use this when the column construct causally increases the value of the row construct in LLM outputs.
   - 'B−': Indicate this when the column construct causally decreases the value of the row construct in LLM outputs.
   - Ensure that all relationships are unidirectional to reflect a clear causal pathway.

2. **Instructions for Filling the Matrix**:
   - Systematically evaluate each construct pair based on their established interrelationships as they pertain to LLM outputs, using accepted theoretical frameworks and interpretations that can be made by a human evaluator with generative AI, information quality, and cognitive mapping methodologies.
   - Determine the type of causal relationship ('A+','A−', 'B+', or 'B−') for each pair and fill only the upper triangular part of the matrix (i.e., the existing NaN will remain unchanged).

3. **Explanation Section**:
   - Provide a brief justification for each relationship listed in the matrix, basing your rationale on recognized information quality principles as they apply to LLM outputs.
   - Each explanation should clearly articulate the expected standard relationship and its impact on LLM-generated content.

4. **Output Formatting Instructions**: 
     - Ensure the response format adheres strictly to the following specifications:
         - **Adjacency Matrix Format**:
           - Begin with the adjacency matrix itself, formatted as a plain-text table. Do not include any headers or introductory text before the matrix.
           - Ensure that each row of the matrix corresponds to a construct, with relationships indicated as per the instructions given in earlier sections.
         - **Separation and Explanation Section**:
           - After completing the matrix, include two line breaks.
           - Then, clearly mark the start of the explanation section with the words **Explanation Section:** followed by another line break.
           - Provide each explanation for the relationships detailed in the matrix immediately following this header. Each explanation should clearly define the causal relationship, supported by a rationale grounded in established principles of information quality as they apply to LLM outputs.
     - These formatting instructions are critical to ensure that the matrix and the subsequent explanations are presented in a clear, concise, and standardized manner. Adherence to this format is essential for the proper documentation and analysis of the information quality dimensions.

This task requires a methodical and precise approach to document how dimensions of information quality interrelate specifically in the context of large language model outputs. Your analysis is essential for establishing a standardized understanding of these relationships.
"""


chat_prompt = ChatPromptTemplate.from_messages([
  ("system", system_prompt),
  ("human", user_prompt),
])



# ------------------------------------------------------------------------------
# Chat Completion Function:
# -------------------------

def generate_chat_completion(temp=0, max_num_tokens=3000):
    """
    Function to generate chat completions, with reasonable defaults for traditional chat completions parameters.
    """
    messages = chat_prompt.format_messages()
    
    if model == 'gpt-4-turbo-preview': 
        chat_model = ChatOpenAI(model="gpt-4-turbo-preview", temperature=temp, max_tokens=max_num_tokens)  
    elif model == 'gpt-3.5-turbo':
        chat_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=temp, max_tokens=max_num_tokens)
    elif model == "claude-3-opus-20240229": 
        chat_model = ChatAnthropic(model="claude-3-opus-20240229", temperature=temp, max_tokens=max_num_tokens)
    elif model == "claude-3-sonnet-20240229":
        chat_model = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=temp, max_tokens=max_num_tokens)
    elif model == "claude-3-haiku-20240307":
        chat_model = ChatAnthropic(model="claude-3-haiku-20240307", temperature=temp, max_tokens=max_num_tokens)
    elif model == "command-r-plus":
        chat_model = ChatCohere(model="command-r-plus", temperature=temp, max_tokens=max_num_tokens)
    elif model == "gemini-pro": 
        chat_model = ChatGoogleGenerativeAI(model="gemini-1.0-pro-latest", convert_system_message_to_human=True, temperature=temp, max_tokens=max_num_tokens)
    elif model == "open-mistral-7b": 
        chat_model = ChatMistralAI(model="open-mistral-7b", temperature=temp, max_tokens=max_num_tokens)
    elif model == "mistral-medium-latest":
        chat_model = ChatMistralAI(model="mistral-medium-latest", temperature=temp, max_tokens=max_num_tokens)
    elif model == "llama3-8b-8192":
        chat_model = ChatGroq(model="llama3-8b-8192", temperature=temp, max_tokens=max_num_tokens)
    elif model == "llama3-70b-8192":
        chat_model = ChatGroq(model="llama3-70b-8192", temperature=temp, max_tokens=max_num_tokens)
    elif model == "gemma-7b-it":
        chat_model = ChatGroq(model="gemma-7b-it", temperature=temp, max_tokens=max_num_tokens)
    else:
        raise ValueError(f"Model {model} is not supported.")  

    # generating the response and extracting the content
    chat_response = chat_model.invoke(messages)
    chat_response_content = chat_response.content

    return chat_response_content



# ------------------------------------------------------------------------------
# Using Chat Models to Populate the Results:
# -----------------------------------------
sample_models = models 
replicates = 5

# Combinations of models and replicates
combinations = list(product(sample_models, range(1, replicates + 1)))

# object to store possible erros
errors = []
dict_of_dfs = {}

# dict to use short names in the key
names_dict = dict(zip(models, short_model_names))

# Looping through the combinations
for model, replicate in combinations:
  try:
    short_model = names_dict[model]
    
    # adding the model and replicate columns
    key = f"{short_model}-rep{replicate}"
    
    # generating the chat completion
    response = generate_chat_completion()
    response += '\n\nGeneration Date: ' + str(dt.datetime.now())
    
    # Saving the response to a text file
    write_path = f"results/info_quality_logs/{key}.txt"
    with open(write_path, 'w') as f:
      f.write(response)
    
    # parsing the response
    lines = response.strip().split('\n')
    # find which line starts with 'A/B,Accuracy,Trustworthness,'
    for i, line in enumerate(lines):
      if line.startswith('A/B,Accuracy,Trustworthness,'):
        lines = lines[i:]
        break
    # find which line starts with Use Intention
    for i, line in enumerate(lines):
      if line.startswith('Use Intention,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN'):
        lines = lines[:i+1]
        break
    
    data = [line.split(',') for line in lines]
    df = pd.DataFrame(data)
    dict_of_dfs[key] = df
    
  except Exception as e:
    # Log any errors that occur
    errors.append((key, str(e)))
  
# Output results or errors for debugging
if errors:
  print("Errors occurred during DataFrame generation:")
  for error in errors:
    print(f"Key: {error[0]}, Error: {error[1]}")
else:
  print("All DataFrames generated successfully.")


# ------------------------------------------------------------------------------
# Saving the Results:
# -------------------
excel_path = 'results/information_quality_completions.xlsx'
with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
  for key, df in dict_of_dfs.items():
    try:
      df.to_excel(writer, sheet_name=key)
    except Exception as e:
      print(f"Failed to write {key} to Excel: {e}")

# save it as pickle (in case parsing needs work)
pickle_path = 'results/information_quality_completions.pkl'
with open(pickle_path, 'wb') as f:
  pickle.dump(dict_of_dfs, f)


# ------------------------------------------------------------------------------
# Clean the Tables for both Models:
# ---------------------------------

headers = ['A/B', 'Accuracy', 'Trustworthness', 'Objectivity', 'Appropriateness', 'Relevance', 'Completeness', 'Conciseness', 'Consistency', 'Underestandability', 'Interpretability', 'Flexibility', 'Satisfaction', 'Use Intention']

# A function to slice the dict of dfs
def slice_dfs(dict_of_dfs):
    sliced_dict = {}
    for key, df in dict_of_dfs.items():
        if len(df.columns) > len(headers):
            # subset the columns to match the headers
            df = df.iloc[:, :len(headers)]
        df.columns = headers
        
        if 'claude' in key:
          df.iloc[1:len(headers)+1, 0] = headers
          
        # Identify start index by matching the header row
        start_index =  df[df['A/B'].str.contains('^(Accuracy|1\. Accuracy)$', na=False)].index[0]
        # Identify end index where first cell is 'Use Intention' and all other cells are NaN
        end_index = df[df['A/B'].str.contains('^(Use Intention|13\. Use Intention)$', na=False)].index[0]
        # Slice the DataFrame from start_index to end_index + 1 to include the 'Use Intention' row and set first column as index
        sliced_dict[key] = df.iloc[start_index:end_index + 1].set_index('A/B')
    return sliced_dict

# Slice the DataFrames to only include the rows from 'Accuracy' to 'Use Intention'
gpt_keys = [key for key in dict_of_dfs.keys() if 'gpt4' in key]
gpt_dfs = {key: dict_of_dfs[key] for key in gpt_keys}
gpt_dfs = slice_dfs(gpt_dfs)

claude_keys = [key for key in dict_of_dfs.keys() if 'claude3' in key]
claude_dfs = {key: dict_of_dfs[key] for key in claude_keys}
claude_dfs = slice_dfs(claude_dfs)


# ------------------------------------------------------------------------------
# Computing the Simple Percent Agreement for Each Model:
# ------------------------------------------------------

combined_gpts = np.array([df.values for df in gpt_dfs.values()], dtype=object)
combined_claudes = np.array([df.values for df in claude_dfs.values()], dtype=object)

# Custom function to calculate mode and percentage agreement
def calculate_percentage_agreement(arr):
    # Initialize arrays to store modelabels and percentage agreement
    mode_labels = np.empty(arr[0].shape, dtype=object)
    percentage_agreement = np.empty(arr[0].shape, dtype=float)

    # Iterate through each element position in the 2D grid
    for i in range(arr[0].shape[0]):
        for j in range(arr[0].shape[1]):
            # Extract the slice of the stack at this position
            elements = arr[:, i, j]
            # Convert None to a unique placeholder string to handle None values
            elements = np.array(['None' if el is None else el for el in elements])
            # Use np.unique to find unique elements and their counts
            values, counts = np.unique(elements, return_counts=True)
            # Find the index of the most frequent element
            max_index = np.argmax(counts)
            # Retrieve the mode label
            mode_labels[i, j] = values[max_index]
            # Calculate the percentage of DataFrames containing the most frequent element
            max_count = counts[max_index]
            percentage_agreement[i, j] = max_count / arr.shape[0] * 100
    
    for i in range(arr[0].shape[0]):
        for j in range(arr[0].shape[1]):
            if i >= j:
                percentage_agreement[i, j] = np.nan

    return mode_labels, percentage_agreement

# Apply the custom agreement calculation
gpts_mode_labels, gpts_perc_agreement = calculate_percentage_agreement(combined_gpts)
gpts_mode_labels = pd.DataFrame(gpt_mode_labels, columns=headers[1:], index=headers[1:])
gpts_perc_agreement = pd.DataFrame(gpts_perc_agreement, columns=headers[1:], index=headers[1:])

claudes_mode_labels, claudes_perc_agreement = calculate_percentage_agreement(combined_claudes)
claudes_mode_labels = pd.DataFrame(claudes_mode_labels, columns=headers[1:], index=headers[1:])
claudes_perc_agreement = pd.DataFrame(claudes_perc_agreement, columns=headers[1:], index=headers[1:])


# ------------------------------------------------------------------------------
# Saving the Results in One Excel File:
# -------------------------------------
with pd.ExcelWriter('results/information_quality_agreements.xlsx', engine='openpyxl') as writer:
    gpts_mode_labels.to_excel(writer, sheet_name='gpt_mode_labels')
    gpts_perc_agreement.to_excel(writer, sheet_name='gpts_perc_agreement')
    claudes_mode_labels.to_excel(writer, sheet_name='claudes_mode_labels')
    claudes_perc_agreement.to_excel(writer, sheet_name='claudes_perc_agreement')


# ------------------------------------------------------------------------------
# Creating a Heatmap for the Agreement:
# -------------------------------------
# A function to create the heatmap for each model
def create_heatmap(df, label_df, model_name, model_print_name):
    plt.figure(figsize=(12, 5))

    # Ensure the values in df are arrays of floats
    numeric_df = df.map(lambda x: np.array(x, dtype=float))

    # Create a custom annotation array that includes both the label and the percentage agreement
    annot_labels = np.asarray([
        ["{0} ({1:.0f}%)".format(label, value) if pd.notna(value) else "{0} (n/a)".format(label)
        for label, value in zip(label_row, value_row)]
      for label_row, value_row in zip(label_df.values, numeric_df.values)
    ])

    bins = [0, 0.01, 20, 40, 60, 80, 100]
    colors = ['#ffffcc', '#d9f0a3', '#addd8e', '#78c679', '#31a354', '#006837']
    cmap = LinearSegmentedColormap.from_list('custom', colors, N=len(bins) - 1)

    # Create the heatmap with seaborn
    heatmap = sns.heatmap(
      numeric_df, annot=annot_labels, fmt="", cmap=cmap, linewidths=0.15,
      linecolor="white",
      annot_kws={"fontsize": 8, "color": "black"},
      vmin=0, vmax=100, mask=numeric_df.isnull(),
      cbar=False,
      # cbar_kws = dict(use_gridspec=False,location="bottom", shrink = 0.5)
      )

    heatmap.set_facecolor('darkgray')
    
    heatmap.xaxis.tick_top()
    heatmap.set_title(f'Mode Label (and Percent Agreement) on Information Quality Relationships for {model_print_name}')
    plt.figtext(0.9, 0.01, f"Model: {model_name} | Generation Date: {dt.datetime.now().date()} | Number of Replicates: {replicates} | Temperature: 0. Grayed Cells refer to NaNs.", ha='right', fontsize=8)

    # Setting smaller tick labels as required
    heatmap.set_xticklabels(heatmap.get_xmajorticklabels(), fontsize=6.5)
    heatmap.set_yticklabels(heatmap.get_ymajorticklabels(), fontsize=6.5)

    stripped_name = model_print_name.lower().replace(' ', '_').replace('-', '_')
    plt.savefig(f'figs/{stripped_name}_perc_agreement.png', dpi=600, bbox_inches='tight')

    return heatmap


# Creating the heatmaps
gpt_heatmap = create_heatmap(gpts_perc_agreement, gpts_mode_labels, 'gpt-4-turbo-preview', 'GPT-4 Turbo')
claude_heatmap = create_heatmap(claudes_perc_agreement, claudes_mode_labels, 'claude-3-opus-20240229', 'Claude 3')


# ------------------------------------------------------------------------------
# Ensemble Agreement Labels and Percentages Across Models:
# --------------------------------------------------------
